{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c81cc28",
   "metadata": {},
   "source": [
    "# Mini Prompting Pipeline for Inference: InternVL2-Llama3-76B\n",
    "\n",
    "following the quick start here:\n",
    "https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B#quick-start\n",
    "or here:\n",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/quick_start.html\n",
    "\n",
    "Structure:\n",
    "1. Split, load (and save) the model (on two 80GB GPUs)\n",
    "2. Preprocess images\n",
    "3. Mini pipeline taking images from a folder and inputting them into the model with the same prompt\n",
    "4. Playground for video input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b718629",
   "metadata": {},
   "source": [
    "## 0. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2df2ba-9cb4-4a29-aa4a-2658433247b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-24.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.37.2\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.37.2)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.37.2)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.37.2)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.37.2)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2022.12.7)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.2 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.15.2 tqdm-4.66.5 transformers-4.37.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting timm\n",
      "  Downloading timm-1.0.11-py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.25.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm\n",
      "Successfully installed timm-1.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting decord\n",
      "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.24.1)\n",
      "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: decord\n",
      "Successfully installed decord-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m155.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "!pip install transformers==4.37.2\n",
    "!pip install timm\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install decord\n",
    "!pip install pandas\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7ee0ef-d286-421c-8e62-568dc1600dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee7161",
   "metadata": {},
   "source": [
    "## 1. Split and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2719a6-2f68-4827-be33-fedf3b4cf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the model on 2 80GB GPUs\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7546a042-4278-4955-b137-a722493d9a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592b7dfaa2e44dcbb23f1248c137c5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80024479cb24286b3d6ee093f321c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_internvl_chat.py:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7b40ce55fb4d54ba26885c9250e53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B:\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B:\n",
      "- configuration_internvl_chat.py\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078e55b3cfed4ea781d528d8f00fe57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_internvl_chat.py:   0%|          | 0.00/15.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e43974249044c988a23ec8ab7445b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conversation.py:   0%|          | 0.00/15.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B:\n",
      "- conversation.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f88582cf5cc4ec08774eac89963cf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_intern_vit.py:   0%|          | 0.00/18.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B:\n",
      "- modeling_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B:\n",
      "- modeling_internvl_chat.py\n",
      "- conversation.py\n",
      "- modeling_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02248c3ba10040cd926fe7601632aec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/122k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4958d33db4144ae5b9892699e880b0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0163142a34bb45beb36d2246b5a1fab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00032.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc21d0277ef64708bafe949efebc8cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00032.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1d42a50f624435b401c5ecba0c1403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00032.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a2cb657df04b149eb2cc2fd290f5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01594e3d658495f8e0a415041f18485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1feef549f9a8465eb3b5a6678c7fc140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a329b685bc7423c863ef993e574c32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00032.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46287ffcdc6489f819d4b1c74649873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00032.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea9c73f16db4dd0b15361409da973db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a6d0ba462b4a99a9f46385be4f1a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86f382084ce4b21a53ea6e8fc8cec88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd0bd1817eb4205a5da48d43b912d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00032.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d84e8392ad42558f0ff270644395b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00032.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4fa5891b464fecb58550b330046a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3256639e234346e7aac1b639bc2e7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20292746978b4b96ab610d29ab39b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6f93ee58db46bb9934a1759f3dd828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00032.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d9e768205c4559b7740f81f1cac960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00032.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6896b1f4ce4efa9ce5e284f520c3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d92a71682fc43e7bbd4e7e38f841fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e1dcd6d37f474991b6c59bffe57f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113d5b5c149a4f23ac6c1af9d845f498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00032.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288000b97f264c7ca43c4a20529b801d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00032.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebe6d2278e848adbbb504543ef23c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac177099ab95429b9cdafd2020c61718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693744c248b3425693843451ba0d3028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45054c588bc34dd090666f774d5c045b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00032.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceba5044c2f94fedbb1dc30b90dd6d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00032.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f98f41069a54a9db278fbacb9f72d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b650776952a5432ba08efdb147413035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00030-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e62189a94ca43abb2af1d8393e4dbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00031-of-00032.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9883020bf6e44d48bbccc2314d18d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00032.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/87/1a/871add617d738b6d30feae10caa6e470085f0bca38ce035108de131780159d79/059cac854f6f17f8f2d7fa1098e5de3a42c78c8c79b6fb1b7ce621d41952a6f7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00032-of-00032.safetensors%3B+filename%3D%22model-00032-of-00032.safetensors%22%3B&Expires=1729415414&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTQxNTQxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzg3LzFhLzg3MWFkZDYxN2Q3MzhiNmQzMGZlYWUxMGNhYTZlNDcwMDg1ZjBiY2EzOGNlMDM1MTA4ZGUxMzE3ODAxNTlkNzkvMDU5Y2FjODU0ZjZmMTdmOGYyZDdmYTEwOThlNWRlM2E0MmM3OGM4Yzc5YjZmYjFiN2NlNjIxZDQxOTUyYTZmNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=frEQ1RoS9X234fNblfZjqKpxaBVrJTS4m9U0DjaHA15pzNDUw2og1oQkvwTJmM01RdeppoQ5mYggHSfgVsBECK-c5PjC9vygSMwisDNVPXndxLDUfetVQYL93mJvbDT4oNq3qysMWKmJB7VhOpNShgu-Wnq9vfQUFDvG5oyKNCV1n-8%7ELUR%7EpFQ4k-SZQPwtwyQz-lubnC2m9v6115ZIMM4uplPifaP2QbRm6WXH9xB8WNgUr9LsOMyv1sY3dsdMkJDxD64cnhthhJyJNCKMkp1YJMaLUCp75Yfw78tBQcuaPwek-PobATzYaBKeDU3Ij%7Ezh5NJnCruavkkYqsWgaA__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c5d1cce74d4a0b9ab3d7af92606608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00032-of-00032.safetensors:  40%|###9      | 1.54G/3.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a868b8e0a595492288ee42a272bfc16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733a83bc75954f58802212560e30f856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8c4b5436be42ec93afececec992bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/57.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939a2eee1824414c8845f20c17b5e7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2638e9d6419340f4ad5410c13de1f06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load the splitted model\n",
    "\n",
    "path = \"OpenGVLab/InternVL2-Llama3-76B\"\n",
    "\n",
    "device_map = split_model('InternVL2-Llama3-76B')\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae530f-3982-4d4b-8614-3b2a3deadc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# save the model\n",
    "\n",
    "model_save_name = \"InternVL2-Llama3-76B.pt\"\n",
    "model_path = \"...\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# load the saved model\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97162b3c-0b0a-49c3-869f-d59cb0ff8b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternVLChatModel(\n",
       "  (vision_model): InternVisionModel(\n",
       "    (embeddings): InternVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 3200, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InternVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-44): 45 x InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear8bitLt(in_features=3200, out_features=9600, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (q_norm): InternRMSNorm()\n",
       "            (k_norm): InternRMSNorm()\n",
       "            (proj): Linear8bitLt(in_features=3200, out_features=3200, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear8bitLt(in_features=3200, out_features=12800, bias=True)\n",
       "            (fc2): Linear8bitLt(in_features=12800, out_features=3200, bias=True)\n",
       "          )\n",
       "          (norm1): InternRMSNorm()\n",
       "          (norm2): InternRMSNorm()\n",
       "          (drop_path1): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128265, 8192)\n",
       "      (layers): ModuleList(\n",
       "        (0-79): 80 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "            (k_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "            (v_proj): Linear8bitLt(in_features=8192, out_features=1024, bias=False)\n",
       "            (o_proj): Linear8bitLt(in_features=8192, out_features=8192, bias=False)\n",
       "            (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear8bitLt(in_features=8192, out_features=28672, bias=False)\n",
       "            (up_proj): Linear8bitLt(in_features=8192, out_features=28672, bias=False)\n",
       "            (down_proj): Linear8bitLt(in_features=28672, out_features=8192, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear8bitLt(in_features=8192, out_features=128265, bias=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): LayerNorm((12800,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear8bitLt(in_features=12800, out_features=8192, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put model in evaluation mode\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dad88",
   "metadata": {},
   "source": [
    "## 2. Preprocessing of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd27ebc-28d8-4622-ac74-5f06efacc36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing the input image\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5e43f",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one example prediction\n",
    "\n",
    "# load image, set the max number of tiles in `max_num`\n",
    "pixel_values = load_image(\"/images/image1.jpg\", max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# give 1 image and text as chat input (single-image single-round conversation) (find good prompt wording, insert template from LEIZA experts)\n",
    "question = '<image>\\nPretend to be an archivist who wants to catalog this photo card digitally. Write a description including different fields which are provided below. Also use the text on the photo card. ...'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'Assistant: \\n {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b2574d2-f5c3-4f11-bbb9-5c38075d5266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0, BK_R16_00011.jpg\n",
      "Assistant: Object title: Fendoch, Perthshire, Scotland\n",
      "Object type: Black and white photograph\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Architecture\n",
      "Detailed motif classification: Military complex\n",
      "Detailed description: Black and white photograph of a military complex in Fendoch, Perthshire, Scotland. The complex consists of several long, rectangular buildings arranged in a grid pattern, with a larger building at the center. The buildings appear to be barracks or similar military structures.\n",
      "Additional information: \n",
      "- FO: Fendoch\n",
      "- Fdst: Not specified\n",
      "- Kreis: Perthshire\n",
      "- Land: Scotland\n",
      "- Zeit: Not specified\n",
      "- Mus: Not specified\n",
      "- Lit: Richmond-McIntyre, Proceedings of the soc. of Antiq. of Scotland, 1930/39, 110 ff.\n",
      "Photo details: \n",
      "- Photographer and origin of photo unknown\n",
      "- 1 photo mounted on photo card\n",
      "- Card labelled by typewriter\n",
      "- Photo discoloured yellow\n",
      "Number of photos: 1\n",
      "Image number: Not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1, BK_R16_00012.jpg\n",
      "Assistant: Object title: View of the Roman London Wall\n",
      "Object type: Colour photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Architecture\n",
      "Detailed motif classification: Fortification\n",
      "Detailed description: Colour photograph of the Roman London Wall, showing a section of the wall with a guard tower and a river in the foreground.\n",
      "Additional information: \n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 2, BK_V85_Latene_L-Z_00072.jpg\n",
      "Assistant: Object title: Iron Age weapons and shields\n",
      "Object type: Black and white photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Object\n",
      "Detailed motif classification: Weapons and shields\n",
      "Detailed description: Black and white photograph of Iron Age weapons and shields. The image shows two round shields with star-like decorations, and below them, a collection of weapons including spears, a sword, and other items.\n",
      "Additional information: \n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 3, Testobjekt_template_chat_GPT.jpg\n",
      "Assistant: Object title: Tree bark\n",
      "Object type: Black and white photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Object\n",
      "Detailed motif classification: Natural object\n",
      "Detailed description: Black and white photograph of tree bark, showing detailed textures and patterns.\n",
      "Additional information: \n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 4, BK_Bestimmung_06202300007.jpg\n",
      "Assistant: Object title: Arch of Constantine\n",
      "Object type: Black and white photograph\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Architecture\n",
      "Detailed motif classification: Arch of honour or triumphal arch\n",
      "Detailed description: Black and white photograph of the Arch of Constantine. The arch is prominently featured in the center of the image, with a clear view through its central opening. Two individuals are standing to the right of the arch, providing a sense of scale.\n",
      "Additional information: chronological classification: Roman; location reference: Rome, Italy; \n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 5, BK_MM_102_Eisenzeit_10420400018.jpg\n",
      "Assistant: Object title: Bronze helmet from the Iron Age\n",
      "Object type: Black and white photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Object\n",
      "Detailed motif classification: Military equipment\n",
      "Detailed description: Black and white photograph of a bronze helmet from the Iron Age. The helmet features a rounded top with a central knob, two curved horns protruding from the sides, and a nose guard. The helmet is displayed against a plain background with a scale on the left side for size reference.\n",
      "Additional information: \n",
      "- FO: Not specified\n",
      "- Fdst: Not specified\n",
      "- Kreis: Not specified\n",
      "- Land: Not specified\n",
      "- Zeit: Iron Age\n",
      "- Mus: Not specified\n",
      "- Lit: Not specified\n",
      "Photo details: \n",
      "- Photographer and origin of photo: Not specified\n",
      "- Number of photos: 1\n",
      "- Image number: LEIZA 1042/04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 6, BK_V85_Latene_L-Z_00074.jpg\n",
      "Assistant: Object title: Bronze fibulae\n",
      "Object type: Black and white photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Object\n",
      "Detailed motif classification: Jewellery\n",
      "Detailed description: Black and white photograph of two bronze fibulae, one with a bird motif and the other with a curved handle.\n",
      "Additional information: \n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 7, BK_V85_Latene_L-Z_00067.jpg\n",
      "Assistant: Object title: Landscape with water and land\n",
      "Object type: Black and white photograph\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Settlements and landscapes\n",
      "Detailed motif classification: Landscape\n",
      "Detailed description: Black and white photograph of a landscape featuring water and land. The water appears calm, and the land has some structures or buildings visible in the distance.\n",
      "Additional information: Not specified\n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 8, BK_V85_Latene_L-Z_00068.jpg\n",
      "Assistant: Object title: Excavation site with workers\n",
      "Object type: Black and white photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: People\n",
      "Detailed motif classification: Group picture\n",
      "Detailed description: Black and white photograph of an excavation site with workers. The workers are seen digging and moving earth, with tools and equipment scattered around the site. The background shows a body of water, possibly a river or lake.\n",
      "Additional information: \n",
      "Photo details: Photographer and origin of photo unknown; 1 photo mounted on photo card; card labelled by hand; photo discoloured yellow; \n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 9, BK_R28_00002.jpg\n",
      "Assistant: Object title: Fragment of a bronzeplastic vessel with a hand holding a bottle\n",
      "Object type: Black and white photography\n",
      "Technique/material: Photographic print\n",
      "Motif classification: Object\n",
      "Detailed motif classification: Sculpture\n",
      "Detailed description: Black and white photograph of a fragment of a bronzeplastic vessel featuring a hand holding a bottle.\n",
      "Additional information: \n",
      "- FO: Bregenz\n",
      "- Fdst: not specified\n",
      "- Kreis: not specified\n",
      "- Land: Österreich\n",
      "- Mus: Vorarlberger Landesmuseum Bregenz\n",
      "- Zeit: not specified\n",
      "- Lit: not specified\n",
      "- Neg: not specified\n",
      "Photo details: \n",
      "- Photographer and origin of photo unknown\n",
      "- 1 photo mounted on photo card\n",
      "- Card labelled by hand\n",
      "- Photo discoloured yellow\n",
      "Number of photos: 1\n",
      "Image number: not specified\n"
     ]
    }
   ],
   "source": [
    "# mini pipeline: generate predictions for all images in the folder and save it in csv\n",
    "\n",
    "# prompt\n",
    "question = '<image>\\nPretend to be an archivist who wants to catalog this photo card digitally. Write a description including different fields which are provided below. Also use the text on the photo card. ...'\n",
    "\n",
    "# images folder\n",
    "image_folder = \"/images\"\n",
    "\n",
    "# dataframe to store image, prompt and response\n",
    "responses = pd.DataFrame(columns=['image', 'prompt', 'response'])\n",
    "\n",
    "i = 0\n",
    "\n",
    "#print(question)\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "  if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):  # Add other extensions if needed\n",
    "\n",
    "    # open image from folder\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "    # load image, set the max number of tiles in `max_num`\n",
    "    pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).cuda()\n",
    "    generation_config = dict(max_new_tokens=1024, do_sample=True, temperature=0.01) # play around with temperature, num_beam and top_k\n",
    "\n",
    "    # give image and text as chat input: single-image single-round conversation\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    print(f'Image: {i}, {filename}\\nAssistant: {response}')\n",
    "    responses = pd.concat([responses, pd.DataFrame({'image': [filename], 'prompt': [question], 'response': [response]})], ignore_index=True)\n",
    "    i += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d992ee0-e529-4763-b19a-356702dd0aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BK_R16_00011.jpg</td>\n",
       "      <td>&lt;image&gt;\\nPretend to be an archivist who wants ...</td>\n",
       "      <td>Object title: Fendoch, Perthshire, Scotland\\nO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BK_R16_00012.jpg</td>\n",
       "      <td>&lt;image&gt;\\nPretend to be an archivist who wants ...</td>\n",
       "      <td>Object title: View of the Roman London Wall\\nO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BK_V85_Latene_L-Z_00072.jpg</td>\n",
       "      <td>&lt;image&gt;\\nPretend to be an archivist who wants ...</td>\n",
       "      <td>Object title: Iron Age weapons and shields\\nOb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Testobjekt_template_chat_GPT.jpg</td>\n",
       "      <td>&lt;image&gt;\\nPretend to be an archivist who wants ...</td>\n",
       "      <td>Object title: Tree bark\\nObject type: Black an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK_Bestimmung_06202300007.jpg</td>\n",
       "      <td>&lt;image&gt;\\nPretend to be an archivist who wants ...</td>\n",
       "      <td>Object title: Arch of Constantine\\nObject type...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image  \\\n",
       "0                  BK_R16_00011.jpg   \n",
       "1                  BK_R16_00012.jpg   \n",
       "2       BK_V85_Latene_L-Z_00072.jpg   \n",
       "3  Testobjekt_template_chat_GPT.jpg   \n",
       "4     BK_Bestimmung_06202300007.jpg   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  <image>\\nPretend to be an archivist who wants ...   \n",
       "1  <image>\\nPretend to be an archivist who wants ...   \n",
       "2  <image>\\nPretend to be an archivist who wants ...   \n",
       "3  <image>\\nPretend to be an archivist who wants ...   \n",
       "4  <image>\\nPretend to be an archivist who wants ...   \n",
       "\n",
       "                                            response  \n",
       "0  Object title: Fendoch, Perthshire, Scotland\\nO...  \n",
       "1  Object title: View of the Roman London Wall\\nO...  \n",
       "2  Object title: Iron Age weapons and shields\\nOb...  \n",
       "3  Object title: Tree bark\\nObject type: Black an...  \n",
       "4  Object title: Arch of Constantine\\nObject type...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model in- and outputs in csv\n",
    "responses_path = \"/responses\"\n",
    "responses.to_csv(responses_path + \"responses.csv\")\n",
    "responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bce007",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70116a03",
   "metadata": {},
   "source": [
    "## 4. Playground for video input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c15e0bfc-562f-434c-be8f-b3dd32f3484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video multi-round conversation\n",
    "\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list\n",
    "\n",
    "# load the video\n",
    "video_path = '/video01.avi'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fead26d3-b7c1-43e3-bf7e-e1f7f17763db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Frame1: <image>\n",
      "Frame2: <image>\n",
      "Frame3: <image>\n",
      "Frame4: <image>\n",
      "Frame5: <image>\n",
      "Frame6: <image>\n",
      "Frame7: <image>\n",
      "Frame8: <image>\n",
      "What is the man playing?\n",
      "Assistant: The man is playing basketball.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Pretend to be a human. Try to detect event boundaries in this video.\n",
      "Assistant: The video shows a man playing basketball in a gym. There are no clear event boundaries in the video,\n"
     ]
    }
   ],
   "source": [
    "question = video_prefix + 'What is the man doing?'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Describe what happens in the video.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
